---
title: "Startup Prediction"
author: "Iryna Popovych, Sofiya Hevorhyan"
date: "April 2019"
output: html_document
---


## Importing data and recording original variables
### Read data
```{r}
# importing train and test data
train <- read.csv(file="./data/CAX_Startup_Train.csv", header=TRUE,as.is=T)
test <- read.csv(file="./data/CAX_Startup_Test.csv", header=TRUE,as.is=T)
train$Category <- c("Train")
test$Category <- c("Test")

# row binding imported train and test data
merged <- rbind(train,test)
```

### Exploring and retyping
We can see that some variables in our data should be retyped. We divide all the independent variables in categories:
1) numerical or integer class, as representation of numbers info, rounded to 4
2) multi-stage factors, as categorical variable
3) 1/0 integer vales as dummy variables
4) CAX_id and dependent variable
```{r}
#sapply(merged, function(x) rbind(class(x), unique(x)))

my.to.factor <- function(x) {
  new.col <- as.factor(x)
  return(new.col)
}

my.to.dummy <- function(x) {
  x <- factor(x, levels=c("No", "Yes"))
  x <- as.numeric(x)-1
  return(x)
}

my.to.number <- function(x) {
  new.column <- round(x, 4)
  return(new.column)
}

my.retype.func <- function(merged) {
  my.names <- names(merged)
  my.cnt.names <- c()
  my.fact.names <- c()
  
  # skip CAX_id and dependent var
  for (i in 3:ncol(merged)) {
    col.class <- class(merged[, i])
    col <- merged[, i]
    
    if (col.class == "numeric") {
      merged[, i] <- my.to.number(col)
      my.cnt.names <- c(my.cnt.names, my.names[i])
      
    } else if (col.class == "character" && unique(col) %in% c("No", "Yes")) {
      merged[, i] <- my.to.dummy(col) 
      my.cnt.names <- c(my.cnt.names, my.names[i])
      
    } else if (col.class == "character") {
      merged[, i] <- my.to.factor(merged[, i])
      my.fact.names <- c(my.fact.names, my.names[i])
      
    } else {
      my.cnt.names <- c(my.cnt.names, my.names[i])
    }
  }
  result <- list()
  result$merged <- merged
  result$my.names <- my.names
  result$my.cnt.names <- my.cnt.names
  result$my.fct.names <- my.fact.names
  
  return(result)
}
```

```{r}
# Note: now all variables are either factors or numbers (except cax_id, 1 col)
# dependent variable is still of type numeric
# names of factor/num col see in result$my.....
result <- my.retype.func(merged)
merged <- result$merged
head(merged)
my.names <- result$my.names

# todo: what to do with categorical var? change to numbers or inlude in model as set of dummies
# todo: remove idcompany from regressions!
```

```{r}
# recoding ordinal variables
# library(plyr)
# merged$employee_count_code<-as.numeric(revalue(merged$Founders_previous_company_employee_count,
# c("Small"=1, "Medium"=2, "Large"=3)))

# recoding with R built-in functions, alternative
# d$employee_count_code[d$Founders_previous_company_employee_count== "Small"] <- 1
# d$employee_count_code[d$Founders_previous_company_employee_count== "Medium"] <- 2
# d$employee_count_code[d$Founders_previous_company_employee_count== "Large"] <- 3

# removing original variable from data frame
# merged$Founders_previous_company_employee_count = NULL
```

## Making Your own test data

```{r}
library(caTools)

my.split <- function(merged) {
  # for return
  result <- list()
  
  train <- merged[merged$Category == "Train",]
  test <- merged[merged$Category == "Test",]
  train$Category <- NULL
  test$Category <- NULL
  
  result$test <- test
  result$train <- train
  
  # partitioning of test and train set for own evaluation of models
  # seprating out 0 and 1 level
  train_0 <- train[train$Dependent==0,]
  train_1 <- train[train$Dependent==1,]
  
  # randomly choosing test and train set for each level
  sample_0 = sample.split(train_0, SplitRatio = .9)
  train_0_new = subset(train_0, sample_0 == TRUE)
  test_0_new = subset(train_0, sample_0 == FALSE)

  sample_1 = sample.split(train_1, SplitRatio = .9)
  train_1_new = subset(train_1, sample_1 == TRUE)
  test_1_new = subset(train_1, sample_1 == FALSE)
  
  # final new train and test set
  result$train_new <- rbind(train_1_new,train_0_new)
  result$test_new <- rbind(test_1_new,test_0_new)
  
  return(result)
}

splitted <- my.split(merged)
test <- splitted$test
train <- splitted$train
train_new <- splitted$train_new
test_new <- splitted$test_new
```

## Graphics

## Variable selection 
### using information value

```{r}
# install developer tool and then woe package from gitub
# refer http://www.r-bloggers.com/r-credit-scoring-woe-information-value-in-woe-package/
# install.packages("devtools")
library(devtools)
# install_github("tomasgreif/woe")
library(woe)
# only dependent and independent variables of training set should be there in data frame
train_new$CAX_ID <- NULL

# calculation of information value
row.names(train_new) <- 1:nrow(train_new) 

IV<-iv.mult(train_new,y="Dependent",TRUE)
# Ignore warnig message for variable which have 0 WOE
# (anyway you will remove these before modeling)

# selecting variables with 0.1<IV<0.5
var<-IV[which(IV$InformationValue>0.1),]
var1<-var[which(var$InformationValue<0.5),]
final_var<-var1$Variable
x_train<-train_new[final_var]
Dependent<-train_new$Dependent
train_final<-cbind(Dependent,x_train)
```

## using randomForest
```{r}
fit_glm=glm(Dependent~.,train_new,family = "binomial")
 
# generate summary
summary(fit_glm)
# Using varImp() function
library(caret)
varImp(fit_glm)
 
#Import the random forest library and fit a model
library(randomForest)
fit_rf=randomForest(Dependent~., data=train_new)
# Create an importance based on mean decreasing gini
importance(fit_rf)
 
# compare the feature importance with varImp() function
varImp(fit_rf)
 
# Create a plot of importance scores by random forest
varImpPlot(fit_rf)
```

## using Mars

```{r}
library(earth)
mars.model <- earth(Dependent ~., data=train_new)
ev <- evimp(mars.model)
x <- ev

# for this moment not in very appropriate form
plot(x =ev,
    cex.var = 1,
    type.nsubsets = "l", col.nsubsets = "black", lty.nsubsets = 1,
    type.gcv = "l", col.gcv = 2, lty.gcv = 1,
    type.rss = "l", col.rss = "gray60", lty.rss = 1,
    cex.legend = 1, x.legend = nrow(x), y.legend = x[1,"nsubsets"],
    rh.col = 1, do.par = F)
```


## PCA


## LDA
```{r}
data.train.lda <- cbind(train_new[, result$my.cnt.names], as.factor(train_new$Dependent))
data.test.lda <- cbind(test_new[, result$my.cnt.names], as.factor(test_new$Dependent))
names(data.train.lda)[38] <- "Dependent"
names(data.test.lda)[38] <- "Dependent"

# install_github("Displayr/flipMultivariates")

# library(flipMultivariates)
# lda <- LDA(class ~ ., data = vehicles)

library(MASS)
lda_model <- lda(Dependent ~., data=data.train.lda)
# shows you the mean, used for LDA
lda_model$means

#Predictions on the test data
lda_pred <- predict(object = lda_model, newdata = data.test.lda)
lda_pred$class

#confusion matrix for lda
library(caret)
confusion.m <- confusionMatrix(lda_pred$class,
                               data.test.lda$Dependent)
confusion.m

qplot(data.test.lda$Dependent,
      lda_pred$class,
      colour= as.numeric(data.test.lda$Dependent), geom = c("boxplot", "jitter"), 
      main = "predicted vs. observed using LDA", 
      xlab = "Observations", ylab = "Predictions") + 
  scale_color_gradientn(colors = c("red", "black"))
```


## SSA




## Model Building

```{r}
# fitting stepwise binary logistic regression with logit link function
mod<-step(glm(Dependent~., family = binomial(link=logit),data = train_final))
# model summary
summary(mod)

# final logistic regression model
model<-glm(formula = Dependent ~ Company_competitor_count +
             Company_1st_investment_time + Founders_Data_Science_skills_score +
             Company_big_data + Founders_publications + Founders_global_exposure,
           family = binomial(link = logit), data = train_final)
# model summary
summary(model)

# odds ratios and 95% CI
exp(cbind(OR = coef(model), confint(model)))

# model fit (Hosmer and Lemeshow goodness of fit (GOF) test)
library(ResourceSelection)
hoslem.test(train_new$Dependent,model$fitted.values, g=10)
```

## Predicting test score and model evaluation

```{r}
# Prediction on test set
pred_prob<-predict (model, newdata=test_new, type="response")
# model accuracy measures
library(ROCR)
pred <- prediction (pred_prob, test_new$Dependent)
# Area under the curve
performance(pred, 'auc')
# creating ROC curve
roc <- performance (pred,"tpr","fpr")
plot(roc)

# create data frame of values
perf <-as.data.frame(cbind(roc@alpha.values[[1]], roc@x.values[[1]], roc@y.values[[1]]))
colnames(perf) <-c("Probability","FPR","TPR")
# removing infinity value from data frame
perf <-perf[-1,]
# reshape the data frame
library(reshape)
perf2<- melt(perf, measure.vars = c("FPR", "TPR"))
# plotting FPR, TPR on y axis and cut-off probability on x axis
library(ggplot2)
ggplot(perf2, aes(Probability, value, colour = variable)) +
geom_line()+ theme_bw()

# model accuracy - Confusion Matrix
library(SDMTools)
confusion.matrix(test_new$Dependent, pred_prob, threshold = 0.42)

# Prediction on test set of CAX
pred_CAX<- predict(model, newdata=test, type="response")
submit_CAX<- cbind(test$CAX_ID,pred_CAX)
colnames(submit_CAX)<- c("CAX_ID", "Dependent")
write.csv(submit_CAX,"Predictions.csv",row.names=F)
```


