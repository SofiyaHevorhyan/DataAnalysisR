---
title: "Startup Prediction"
author: "Iryna Popovych, Sofiya Hevorhyan"
date: "April 2019"
output: html_notebook
---

*This project is a second-year semester work on course "Linear Algebra". The aim of this research is to predict whether the startup will be successful or not with the highest accuracy, based on different criteria. Because of this purpose were used a couple of different methods for variable selection along with Principal Component Analysis and Linear Discriminant Analysis*

## Importing data and recording original variables
### Read data
First of all, we have to read the data and store it in appropriate variables. There are also some further transformation with it, so we add column Category and merged dataset so we will not perform the same actions twice
```{r}
# importing train and test data
train <- read.csv(file="./data/CAX_Startup_Train.csv", header=TRUE,as.is=T)
test <- read.csv(file="./data/CAX_Startup_Test.csv", header=TRUE,as.is=T)
train$Category <- c("Train")
test$Category <- c("Test")

# row binding imported train and test data
merged <- rbind(train,test)
```

### Exploring and retyping
We can see that some variables in our data should be retyped. We divide all the variables in categories:
1) numerical or integer class, as representation of numbers info, rounded to 4
2) multi-stage factors, as categorical variable
3) 1/0 integer vales as dummy variables
4) CAX_id and dependent variable
```{r}
my.to.factor <- function(x) {
  new.col <- as.factor(x)
  return(new.col)
}

my.to.dummy <- function(x) {
  x <- factor(x, levels=c("No", "Yes"))
  x <- as.numeric(x)-1
  return(x)
}

my.to.number <- function(x) {
  new.column <- round(x, 4)
  return(new.column)
}

my.retype.func <- function(merged) {
  my.names <- names(merged)
  my.cnt.names <- c()
  my.fact.names <- c()
  
  # skip CAX_id and dependent var
  for (i in 3:ncol(merged)) {
    col.class <- class(merged[, i])
    col <- merged[, i]
    
    if (col.class == "numeric") {
      merged[, i] <- my.to.number(col)
      my.cnt.names <- c(my.cnt.names, my.names[i])
      
    } else if (col.class == "character" && unique(col) %in% c("No", "Yes")) {
      merged[, i] <- my.to.dummy(col) 
      my.cnt.names <- c(my.cnt.names, my.names[i])
      
    } else if (col.class == "character") {
      merged[, i] <- my.to.factor(merged[, i])
      my.fact.names <- c(my.fact.names, my.names[i])
      
    } else {
      my.cnt.names <- c(my.cnt.names, my.names[i])
    }
  }
  result <- list()
  result$merged <- merged
  result$my.names <- my.names
  result$my.cnt.names <- my.cnt.names
  result$my.fct.names <- my.fact.names
  
  return(result)
}
```
Retyping some of the variables by different functions
```{r}
# Note: now all variables are either factors or numbers (except cax_id, 1 col)
# dependent variable is still of type numeric
# names of factor/num col see in result$my.....
result <- my.retype.func(merged)
merged <- result$merged
head(merged)
my.names <- result$my.names

# todo: what to do with categorical var? change to numbers or inlude in model as set of dummies
```

```{r}
# recoding ordinal variables
# library(plyr)
# merged$employee_count_code<-as.numeric(revalue(merged$Founders_previous_company_employee_count,
# c("Small"=1, "Medium"=2, "Large"=3)))

# recoding with R built-in functions, alternative
# d$employee_count_code[d$Founders_previous_company_employee_count== "Small"] <- 1
# d$employee_count_code[d$Founders_previous_company_employee_count== "Medium"] <- 2
# d$employee_count_code[d$Founders_previous_company_employee_count== "Large"] <- 3

# removing original variable from data frame
# merged$Founders_previous_company_employee_count = NULL
```

## Making Your own test data
To perform different analysis we have split our train data into two sets - train_new to train our models (using different methods) and to test them with test_new (as in original test we don't have real values of Dependent column)
```{r}
library(caTools)

my.split <- function(merged) {
  # for return
  result <- list()
  
  train <- merged[merged$Category == "Train",]
  test <- merged[merged$Category == "Test",]
  train$Category <- NULL
  test$Category <- NULL
  
  result$test <- test
  result$train <- train
  
  # partitioning of test and train set for own evaluation of models
  # seprating out 0 and 1 level
  train_0 <- train[train$Dependent==0,]
  train_1 <- train[train$Dependent==1,]
  
  # randomly choosing test and train set for each level
  sample_0 = sample.split(train_0, SplitRatio = .9)
  train_0_new = subset(train_0, sample_0 == TRUE)
  test_0_new = subset(train_0, sample_0 == FALSE)

  sample_1 = sample.split(train_1, SplitRatio = .9)
  train_1_new = subset(train_1, sample_1 == TRUE)
  test_1_new = subset(train_1, sample_1 == FALSE)
  
  # final new train and test set
  result$train_new <- rbind(train_1_new,train_0_new)
  result$test_new <- rbind(test_1_new,test_0_new)
  
  return(result)
}

splitted <- my.split(merged)
test <- splitted$test
train <- splitted$train
train_new <- splitted$train_new
test_new <- splitted$test_new
```

## Graphics

```{r}
# todo: more graphics!!
library(ggvis)

# Iris scatter plot
train %>% ggvis(~Company_senior_team_count, ~Founders_skills_score, fill = ~Dependent) %>% layer_points()
```


## Variable selection

### using information value
First, we try to select some important variables using information value method. We will remove CAX_id column so it will not influence our results as only dependent and independent variables should be included in data frame
```{r}
# install.packages("devtools")
library(devtools)
# install_github("tomasgreif/woe")
library(woe)

train_new$CAX_ID <- NULL

# calculation of information value
row.names(train_new) <- 1:nrow(train_new) 
IV <- iv.mult(train_new,y="Dependent",TRUE)

# selecting variables with 0.1 < IV < 0.5
var1 <- IV[which(IV$InformationValue>0.1),]
var1 <- c(var1, var1[which(var1$InformationValue<0.5),])
x_train <- train_new[var1$Variable]
Dependent <- train_new$Dependent
# final df with important variables
train1 <- cbind(Dependent,x_train)
```

### using randomForest
Another methods for variables selection are randomForest and variable importance from caret package
```{r}
# generate summary
fit_glm=glm(Dependent~.,train_new,family = "binomial")
summary(fit_glm)

# Using varImp() function
library(caret)
importance1 <- varImp(fit_glm)
 
#Import the random forest library and fit a model
library(randomForest)
fit_rf=randomForest(Dependent~., data=train_new)
# Create an importance based on mean decreasing gini
importance2 <- importance(fit_rf)
 
# Create a plot of importance scores by random forest
varImpPlot(fit_rf)

# select variables for second model
# 2.5 as a key point before sharp decline (top-7 variables)
var2 <- rownames(importance1)[which(importance1$Overall > 2.5)]
var2[var2 == "Founders_publicationsNone"] <- "Founders_publications"
var2[var2 == "Founder_educationMasters"] <- "Founder_education"
var2 <- c(var2, rownames(importance2)[which(importance2 > 2.5)])
# final df from rf
train2 <- cbind(Dependent, train_new[,var2[!var2 %in% "Company_business_modelB2C"]])
```

### using Mars

```{r}
library(earth)
mars.model <- earth(Dependent ~., data=train_new)
ev <- evimp(mars.model)

# for this moment not in very appropriate form
plot(x=ev,
    cex.var = 1,
    type.nsubsets = "l", col.nsubsets = "black", lty.nsubsets = 1,
    type.gcv = "l", col.gcv = 2, lty.gcv = 1,
    type.rss = "l", col.rss = "gray60", lty.rss = 1,
    cex.legend = 1, x.legend = nrow(ev), y.legend = ev[1,"nsubsets"],
    rh.col = 1, do.par = F)

# todo: interpret results
var3 <- rownames(ev)
var3[var3 == "Company_business_modelB2C"] <- "Company_business_model"
var3[var3 == "Founders_skills_score-unused"] <- "Founders_skills_score"
train3 <- cbind(Dependent, train_new[, var3])
```


## PCA


## LDA
Our next method for analyzing our data in Linear Discriminant Analysis. For this method we need only numeric or integer data types as independent variables and categorical as dependent
```{r}
data.train.lda <- cbind(train_new[, result$my.cnt.names], as.factor(train_new$Dependent))
data.test.lda <- cbind(test_new[, result$my.cnt.names], as.factor(test_new$Dependent))
names(data.train.lda)[38] <- "Dependent"
names(data.test.lda)[38] <- "Dependent"

library(MASS)
lda_model <- lda(Dependent ~., data=data.train.lda)
# shows you the mean, used for LDA
lda_model$means

#Predictions on the test data
lda_pred <- predict(object = lda_model, newdata = data.test.lda)
lda_pred$class

#confusion matrix for lda
library(caret)
confusion.m <- confusionMatrix(lda_pred$class,
                               data.test.lda$Dependent)
confusion.m

qplot(data.test.lda$Dependent,
      lda_pred$class,
      colour= as.numeric(data.test.lda$Dependent), geom = c("boxplot", "jitter"), 
      main = "predicted vs. observed using LDA", 
      xlab = "Observations", ylab = "Predictions") + 
  scale_color_gradientn(colors = c("red", "black"))
```


## KNN
```{r}
# one way
library(caret)
model_knn <- train(train_new[, result$my.cnt.names], as.factor(train_new[, 1]), method='knn')
predictions<-predict(object=model_knn,test_new[,result$my.cnt.names])

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,as.factor(test_new$Dependent))


# takoe
# sec way
# Setting up train controls
# repeats = 3
# numbers = 10
# tunel = 10
# 
# x = trainControl(method = "repeatedcv",
#                  number = numbers,
#                  repeats = repeats,
#                  classProbs = TRUE,
#                  summaryFunction = twoClassSummary)
# data.train.knn <- data.train.lda
# levels(data.train.knn$Dependent) <- c("f", "s")
# model1 <- train(Dependent~. , data = data.train.knn, method = "knn",
#                preProcess = c("center","scale"),
#                trControl = x,
#                metric = "ROC",
#                tuneLength = tunel)
# 
# # Summary of model
# model1
# plot(model1)
# 
# # Validation
# valid_pred <- predict(model1,test_new, type = "prob")
# table(valid_pred$f)
# round(valid_pred$f)
# length(test_new)
# # Confusion matrix 
# confusionMatrix(as.factor(round(valid_pred$s)),
#                 as.factor(test_new$Dependent))
# 

# todo: interpret results
``` 

## Model Building

```{r}
# fitting stepwise binary logistic regression with logit link function
mod1 <- step(glm(Dependent~., family = binomial(link=logit),data = train1))

# from randomForest
mod2 <- step(glm(Dependent~., family = binomial(link=logit),data = train2))

# from mars
mod3 <- step(glm(Dependent~., family = binomial(link=logit),data = train3))

summary(mod1)
summary(mod2)
summary(mod3)
# todo: conf.m for all of them



# ????
# odds ratios and 95% CI
exp(cbind(OR = coef(model), confint(model)))

# model fit (Hosmer and Lemeshow goodness of fit (GOF) test)
library(ResourceSelection)
hoslem.test(train_new$Dependent,model$fitted.values, g=10)
```

## Predicting test score and model evaluation

```{r}
# Prediction on test set
pred_prob<-predict (model, newdata=test_new, type="response")
# model accuracy measures
library(ROCR)
pred <- prediction (pred_prob, test_new$Dependent)
# Area under the curve
performance(pred, 'auc')
# creating ROC curve
roc <- performance (pred,"tpr","fpr")
plot(roc)

# create data frame of values
perf <-as.data.frame(cbind(roc@alpha.values[[1]], roc@x.values[[1]], roc@y.values[[1]]))
colnames(perf) <-c("Probability","FPR","TPR")
# removing infinity value from data frame
perf <-perf[-1,]
# reshape the data frame
library(reshape)
perf2<- melt(perf, measure.vars = c("FPR", "TPR"))
# plotting FPR, TPR on y axis and cut-off probability on x axis
library(ggplot2)
ggplot(perf2, aes(Probability, value, colour = variable)) +
geom_line()+ theme_bw()

# model accuracy - Confusion Matrix
library(SDMTools)
confusion.matrix(test_new$Dependent, pred_prob, threshold = 0.42)

# Prediction on test set of CAX
pred_CAX<- predict(model, newdata=test, type="response")
submit_CAX<- cbind(test$CAX_ID,pred_CAX)
colnames(submit_CAX)<- c("CAX_ID", "Dependent")
write.csv(submit_CAX,"Predictions.csv",row.names=F)
```


