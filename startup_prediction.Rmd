---
title: "Startup Prediction"
author: "Iryna Popovych, Sofiya Hevorhyan"
date: "April 2019"
output: html_notebook
---

*This project is a second-year semester work on course "Linear Algebra". The aim of this research is to predict whether the startup will be successful or not with the highest accuracy, based on different criteria. Because of this purpose were used a couple of different methods for variable selection along with Principal Component Analysis, Linear Discriminant Analysis and Support Vector Machine*

## Importing data and recording original variables
### Read data
First of all, we have to read the data and store it in appropriate variables. There are also some further transformation with it, so we add column Category and merge dataset so we will not perform the same actions twice
```{r}
set.seed(500)

# importing train and test data
train <- read.csv(file="./data/CAX_Startup_Train.csv", header=TRUE,as.is=T)
test <- read.csv(file="./data/CAX_Startup_Test.csv", header=TRUE,as.is=T)
train$Category <- c("Train")
test$Category <- c("Test")

# row binding imported train and test data
merged <- rbind(train,test)
```

### Exploring and retyping
We can see that some variables in our data should be retyped. We divide all the variables in categories:
1) numerical or integer class, as representation of numbers info, rounded to 4
2) multi-stage factors, as categorical variable
3) 1/0 integer values as dummy variables
4) CAX_id and dependent variable
```{r}
my.to.factor <- function(x) {
  new.col <- as.factor(x)
  return(new.col)
}

my.to.dummy <- function(x) {
  x <- factor(x, levels=c("No", "Yes"))
  x <- as.numeric(x)-1
  return(x)
}

my.to.number <- function(x) {
  new.column <- round(x, 4)
  return(new.column)
}

my.retype.func <- function(merged) {
  my.names <- names(merged)
  my.cnt.names <- c()
  my.fact.names <- c()
  
  # skip CAX_id and dependent var
  for (i in 3:ncol(merged)) {
    col.class <- class(merged[, i])
    col <- merged[, i]
    
    if (col.class == "numeric") {
      merged[, i] <- my.to.number(col)
      my.cnt.names <- c(my.cnt.names, my.names[i])
      
    } else if (col.class == "character" && unique(col) %in% c("No", "Yes")) {
      merged[, i] <- my.to.dummy(col) 
      my.cnt.names <- c(my.cnt.names, my.names[i])
      
    } else if (col.class == "character") {
      merged[, i] <- my.to.factor(merged[, i])
      my.fact.names <- c(my.fact.names, my.names[i])
      
    } else {
      my.cnt.names <- c(my.cnt.names, my.names[i])
    }
  }
  result <- list()
  result$merged <- merged
  result$my.names <- my.names
  result$my.cnt.names <- my.cnt.names
  result$my.fct.names <- my.fact.names
  
  return(result)
}
```
Retyping some of the variables by different functions
```{r}
# Note: now all variables are either factors or numbers (except cax_id, 1 col)
# dependent variable is still of type numeric
# names of factor/num col see in result$my.....
result <- my.retype.func(merged)
merged <- result$merged
head(merged)
my.names <- result$my.names
```

## Making Your own test data
To perform different analysis we have splitted our train data into two sets - train_new to train our models (using different methods) and to test them with test_new (as in original test we don't have real values of Dependent column)
```{r}
library(caTools)

my.split <- function(merged) {
  # for return
  result <- list()
  
  train <- merged[merged$Category == "Train",]
  test <- merged[merged$Category == "Test",]
  train$Category <- NULL
  test$Category <- NULL
  
  result$test <- test
  result$train <- train
  
  # partitioning of test and train set for own evaluation of models
  # seprating out 0 and 1 level
  train_0 <- train[train$Dependent==0,]
  train_1 <- train[train$Dependent==1,]
  
  # randomly choosing test and train set for each level
  sample_0 = sample.split(train_0, SplitRatio = .9)
  train_0_new = subset(train_0, sample_0 == TRUE)
  test_0_new = subset(train_0, sample_0 == FALSE)

  sample_1 = sample.split(train_1, SplitRatio = .9)
  train_1_new = subset(train_1, sample_1 == TRUE)
  test_1_new = subset(train_1, sample_1 == FALSE)
  
  # final new train and test set
  result$train_new <- rbind(train_1_new,train_0_new)
  result$test_new <- rbind(test_1_new,test_0_new)
  
  return(result)
}

splitted <- my.split(merged)
test <- splitted$test
train <- splitted$train
train_new <- splitted$train_new
test_new <- splitted$test_new
```

## Graphics
After retyping and transformation, we tried to play with data to understand it better and reveal some of our intuitive ideas and predictions that might have some effect in our analysis

```{r}
# install.packages('ggvis')
library(ggvis)
library(ggplot2)

# first plot, scatter 
train %>% ggvis(~Founders_skills_score, ~Company_senior_team_count, fill = ~Dependent) %>% layer_points()

# second plot
ggplot(train_new,
       aes(x=as.factor(train_new$Dependent),
           y=Company_senior_team_count,
                   fill=as.factor(train_new$Dependent))) +
  xlab("Startup status") +
  geom_boxplot()

# third plot
ggplot(train_new, aes(x=train_new$Company_avg_investment_time)) + 
  geom_histogram(aes(fill=..count..), binwidth = 0.7) +
  scale_x_continuous(name="avg investment time",
                     breaks=seq(1, 20, by=1), limits=c(1, 20)) +
  scale_y_continuous(name="number of startups", limits=c(0,30)) +
  ggtitle("Distribution of company avr investment time") +
  theme_bw() +
  geom_vline(xintercept = mean(train_new$Company_avg_investment_time), size = 1,
             colour = "#FF3721",
             linetype = "dashed")
```


## Variable selection

One last data transformation
```{r}
# to not spoil the data
train_new$CAX_ID <- NULL
test_new$CAX_ID <- NULL

library(dummies)
# before moving to var selecion
# create data frame with dummy variables and all numerical
# now that's all independent variables
pca.train <- dummy.data.frame(subset(train_new, select = -Dependent), names = result$my.fct.names)
pca.test <- dummy.data.frame(subset(test_new, select = -Dependent), names = result$my.fct.names)
pca.test.final <- dummy.data.frame(subset(test, select = -Dependent), names = result$my.fct.names)
# names(pca.test.final)[!names(pca.test.final) %in% names(pca.test)] 

# now we have 75 variables including dummies
pca.train
pca.test

# here we select only those columns for training which we have in test, it it the pre-requirement for the model
pca.train <- subset(pca.train, select=c(colnames(pca.test)))
pca.test.final <- subset(pca.test.final, select=c(colnames(pca.test), "CAX_ID"))
# now train has the same numer of columns as test
pca.train

data.train.all <- cbind(pca.train, as.factor(train_new$Dependent))
data.test.all <- cbind(pca.test, as.factor(test_new$Dependent))
data.test.final <- cbind(pca.test.final, as.factor(test$Dependent))
names(data.train.all)[75] <- "Dependent"
names(data.test.all)[75] <- "Dependent"
names(data.test.final)[76] <- "Dependent"
levels(data.train.all$Dependent) <- c("fail", "success")
levels(data.test.all$Dependent) <- c("fail", "success")
levels(data.test.final$Dependent) <- c("fail", "success")
```
### using information value
First, we try to select some important variables using information value method. We will remove CAX_id column so it will not influence our results as only dependent and independent variables should be included in data frame
Information Value for logistic regression is analogous to correlation for linear regression. It tells us how well an independent variable is able to distinguish two categories of dependent variables
```{r}
# install.packages("devtools")
library(devtools)
# install_github("tomasgreif/woe")
library(woe)

# calculation of information value
row.names(data.train.all) <- 1:nrow(data.train.all) 
IV <- iv.mult(data.train.all,y="Dependent",TRUE)

# selecting variables with 0.1 < IV < 0.5
var1 <- IV[which(IV$InformationValue>0.1),]
var1 <- c(var1, var1[which(var1$InformationValue<0.5),])
x_train <- data.train.all[var1$Variable]
Dependent <- data.train.all$Dependent

# final df with important variables
train.iv <- cbind(Dependent,x_train)
as.data.frame(train.iv)
```

### using randomForest
Another methods for variables selection are randomForest and variable importance from caret package. Random Forest is one of the most popular classification algoritms. Our goal is to pick up some relevant features that will classify our Dependent variable well.
```{r}
# generate summary
fit_glm=glm(Dependent~.,data.train.all,family = "binomial")
summary(fit_glm)

# Using varImp() function
library(caret)
importance1 <- varImp(fit_glm)
 
#Import the random forest library and fit a model
library(randomForest)
fit_rf=randomForest(Dependent~., data=data.train.all)

# Create an importance based on mean decreasing gini
importance2 <- importance(fit_rf)
 
# Create a plot of importance scores by random forest
varImpPlot(fit_rf)

# select variables for second model
# 2.5 as a key point before sharp decline (top-7 variables)
var2 <- rownames(importance1)[which(importance1$Overall > 2.4)]
# var2[var2 == "Founders_publicationsNone"] <- "Founders_publications"
# var2[var2 == "Company_LocationUSA"] <- "Company_Location"
# var2[var2 == "Founder_educationMasters"] <- "Founder_education"
# var2[var2 == "Company_business_modelB2C"] <- "Company_business_model"
var2 <- c(var2, rownames(importance2)[which(importance2 > 3)])

# final df from rf
train.varimp <- cbind(Dependent, data.train.all[,var2])
as.data.frame(train.varimp)
```

### using Mars
MARS is another way to pick up variables for our regression. It implements variable importance based on Generalized cross validation (GCV), number of subset models the variable occurs (nsubsets) and residual sum of squares (RSS).
```{r}
library(earth)
mars.model <- earth(Dependent ~., data=data.train.all)
ev <- evimp(mars.model)

# for this moment not in very appropriate form
plot(x=ev,
    cex.var = 1,
    type.nsubsets = "l", col.nsubsets = "black", lty.nsubsets = 1,
    type.gcv = "l", col.gcv = 2, lty.gcv = 1,
    type.rss = "l", col.rss = "gray60", lty.rss = 1,
    cex.legend = 1, x.legend = nrow(ev), y.legend = ev[1,"nsubsets"],
    rh.col = 1, do.par = F)

var3 <- rownames(ev)

# var3[var3 == "Company_business_modelB2C"] <- "Company_business_model"
# var3[var3 == "Founders_skills_score-unused"] <- "Founders_skills_score"
# var2[var2 == "Founders_publicationsNone"] <- "Founders_publications"
# var3[var3 == "Founder_educationMasters"] <- "Founder_education"
# var3[var3 == "Company_crowdfunding-unused"] <- "Company_crowdfunding"

#final df from mars
train.mars <- cbind(Dependent, data.train.all[, var3])
as.data.frame(train.mars)
```

## PCA

### PCA with all data
Let's first try to perform PCA using all the numerical variables we have, to see whetther our data could be spread into components.
```{r}
train_new
```

```{r}
# principal component analysis
prin_comp <- prcomp(pca.train, scale. = T)
# take a brief look at top components and variables rotation matrix
prin_comp$rotation[1:5, 1:4]

library(ggbiplot)
#plot PCA with vectors
ggbiplot(prin_comp)
#plot 1st and 2nd principal components grouping by Industry Exposure of Founders
ggbiplot(prin_comp, groups=train_new$Founders_Industry_exposure, scale = 0, ellipse=TRUE)

#compute standard deviation of each principal component
std_dev <- prin_comp$sdev

#compute variance
pr_var <- std_dev^2

#proportion of variance explained
prop_varex <- pr_var/sum(pr_var)
prop_varex[1:20]
```

First principal component explains 7.3% variance. Second component explains 5.1% variance. Third component explains 4.4% variance and so on. So, how do we decide how many components should we select for modeling stage?

```{r}
#scree plot
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")

```

The plot above shows that ~ 60 components explain more than 98% variance in the data set. In order words, using PCA we have reduced 75 predictors to 60 without compromising on explained variance. This is the power of PCA. Let’s do a confirmation check, by plotting a cumulative variance plot. This will give us a clear picture of number of components.
```{r}
#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
```
This plot shows that 60 components results in variance close to ~ 98%. Therefore, in this case, we’ll select number of components as 60 [PC1 to PC60] and proceed to the modeling stage. This completes the steps to implement PCA on train data. For modeling, we’ll use these 60 components as predictor variables. See this later in Notebook, after the SVM part.


## LDA
Our next method for analyzing our data in Linear Discriminant Analysis. For this method we need only numeric or integer data types as independent variables and categorical as dependent
```{r}
library(MASS)
lda_model <- lda(Dependent ~., data=data.train.all)
# shows you the mean, used for LDA
# head(lda_model$means)

#Predictions on the test data
lda_pred <- predict(object = lda_model, newdata = data.test.all)
lda_pred$class

#confusion matrix for lda
library(caret)
confusion.m.lda <- confusionMatrix(lda_pred$class,
                                   data.test.all$Dependent)
confusion.m.lda

qplot(data.test.all$Dependent,
      lda_pred$class,
      colour= as.numeric(data.test.all$Dependent)-1, geom = c("boxplot", "jitter"), 
      main = "predicted vs. observed using LDA", 
      xlab = "Observations", ylab = "Predictions") + 
  scale_color_gradientn(colors = c("blue", "black"))
```


## KNN
KNN or K-Nearest Neighbor is another learning technique where we try to classify data to a given category. For the new observation x, prediction is made by searching on the whole data set k-nearest neighbors (most similar cases) and on based on this the category of x is determined
```{r}
# First of all, we need to find the optimal k for our model
# Setting up train controls
repeats = 3
numbers = 10
tunel = 10

x = trainControl(method = "repeatedcv",
                 number = numbers,
                 repeats = repeats,
                 classProbs = TRUE,
                 summaryFunction = twoClassSummary)

model.knn <- train(Dependent~. , data = data.train.all, method = "knn",
                   preProcess = c("center","scale"),
                   trControl = x,
                   metric = "ROC",
                   tuneLength = tunel)

# Summary of model
model.knn
plot(model.knn)

# Validation
valid_pred <- predict(model.knn, data.test.all, type = "prob")

# Confusion matrix
predicted.knn <- as.factor(round(valid_pred$success)) 
levels(predicted.knn) <- c("fail", "success")
confusion.m.knn <- confusionMatrix(predicted.knn,
                                   data.test.all$Dependent)
confusion.m.knn

qplot(data.test.all$Dependent,
      predicted.knn,
      colour= as.numeric(data.test.all$Dependent)-1, geom = c("boxplot", "jitter"), 
      main = "predicted vs. observed using KNN", 
      xlab = "Observations", ylab = "Predictions") + 
  scale_color_gradientn(colors = c("purple", "black"))

``` 

## Model Building
### Logit Models
We build three logistic regression models out of components that we obtained using differend methods for feature selection. By doing this, we'll able to compare these different approaches.
```{r}
# fitting stepwise binary logistic regression with logit link function, using features selected with Information value
mod.iv <- step(glm(Dependent~., family = binomial(link=logit),data = train.iv))

# from randomForest
mod.varimp <- step(glm(Dependent~., family = binomial(link=logit),data = train.varimp))

# from mars
mod.mars <- step(glm(Dependent~., family = binomial(link=logit),data = train.mars))

summary(mod.iv)
summary(mod.varimp)
summary(mod.mars)

#predict
iv.pred <- predict(object=mod.iv, newdata=data.test.all, type="response")
iv.pred <- as.factor(round(iv.pred))
levels(iv.pred) <- c("fail", "success")

varimp.pred <- predict(object=mod.varimp, newdata=data.test.all, type="response")
varimp.pred <- as.factor(round(varimp.pred))
levels(varimp.pred) <- c("fail", "success")

#confusion matrix for iv
confusion.m.iv <- confusionMatrix(iv.pred, data.test.all$Dependent)
confusion.m.iv
qplot(data.test.all$Dependent,
      iv.pred,
      colour= as.numeric(data.test.all$Dependent)-1, geom = c("boxplot",
                                                              "jitter"), 
      main = "predicted vs. observed using IV", 
      xlab = "Observations", ylab = "Predictions") + 
  scale_color_gradientn(colors = c("red", "black"))

#confusion matrix for varimp
confusion.m.varimp <- confusionMatrix(varimp.pred, data.test.all$Dependent)
confusion.m.varimp
qplot(data.test.all$Dependent,
      varimp.pred,
      colour= as.numeric(data.test.all$Dependent)-1, geom = c("boxplot", "jitter"), 
      main = "predicted vs. observed using RandForest", 
      xlab = "Observations", ylab = "Predictions") + 
  scale_color_gradientn(colors = c("blue", "black"))



# model fit (Hosmer and Lemeshow goodness of fit (GOF) test)
library(ResourceSelection)
hoslem.test(train_new$Dependent,mod.iv$fitted.values, g=10)
```

### Support Verctor Machine
We take the dataframe with the features selected using Information Value algorithm and use these selected features to build an SVM model for predicting binary outcome.
```{r}
train.iv
```


```{r}
library('e1071')

# building SVM model from the variables obtained after feature selection using IV.

model.svm <- svm(Dependent ~  .,
              data = train.iv, probability = TRUE, type='nu-classification')


print(model.svm)

library(caret)
confusionMatrix(as.factor(train.iv$Dependent), predict(model.svm))


#plot model
plot(model.svm, data.train.all, Founders_Data_Science_skills_score ~ Company_avg_investment_time,
     svSymbol = 1, dataSymbol = 2, symbolPalette = rainbow(4),
color.palette = terrain.colors)
```


### SVM on principal components
```{r}
#add a training set with principal components
train.data <- data.frame(Dependent = train_new$Dependent, prin_comp$x)

#we are interested in first 60 PCAs
train.data <- train.data[,1:61]

#transform test into PCA
test.data <- predict(prin_comp, newdata = pca.test)
test.data <- as.data.frame(test.data)

#select the first 60 components
test.data <- test.data[,1:61]


library('e1071')

# building SVM model from the variables obtained after feature selection using PCA.
model.svm.pca <- svm(Dependent ~ .,data = train.data, probability = TRUE, type='nu-classification')

#make prediction on test data
svm.prediction <- predict(model.svm.pca, test.data, type = "class")


confusionMatrix(as.factor(test_new$Dependent), svm.prediction)
```


## Predicting test score and model evaluation
```{r}
# Prediction on test set of CAX

# for PCA model
# test
# x <- subset(test, select = -Dependent)
# pca.test <- dummy.data.frame(subset(test, select = -Dependent), names = result$my.fct.names)
# 
# #transform test into PCA
# test.data <- predict(prin_comp, newdata = pca.test)
# test.data <- as.data.frame(test.data)
# 
# #select the first 60 components
# x <- test.data[,1:61]
# 
# 
# pred_CAX <- predict(model.svm.pca, newdata=x)


iv.pred.final <- predict(object=mod.iv, newdata=data.test.final, type="response")
iv.pred.final
submit_iv.pred <- cbind(test$CAX_ID, round(iv.pred.final))
colnames(submit_iv.pred)<- c("CAX_ID", "Dependent")
write.csv(submit_iv.pred,"PredictionsIV.csv",row.names=F)


varimp.pred.final <- predict(object=mod.varimp, newdata=data.test.final, type="response")
varimp.pred.final
submit_varimp.pred <- cbind(test$CAX_ID, round(varimp.pred.final))
colnames(submit_varimp.pred)<- c("CAX_ID", "Dependent")
write.csv(submit_varimp.pred,"PredictionsVARIMP.csv",row.names=F)


knn.pred.final <- predict(model.knn, data.test.final, type = "prob")
knn.pred.final$success
submit_knn.pred <- cbind(test$CAX_ID, round(knn.pred.final$success))
colnames(submit_knn.pred)<- c("CAX_ID", "Dependent")
write.csv(submit_knn.pred,"PredictionsKNN.csv",row.names=F)

lda_pred.final <- predict(object = lda_model, newdata = data.test.final)
lda_pred.final$class
submit_lda.pred <- cbind(test$CAX_ID, as.numeric(lda_pred.final$class)-1)
colnames(submit_lda.pred)<- c("CAX_ID", "Dependent")
write.csv(submit_lda.pred,"PredictionsLDA.csv",row.names=F)

```